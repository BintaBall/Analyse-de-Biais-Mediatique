import torch
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.neighbors import NearestNeighbors
import matplotlib.pyplot as plt
import warnings
from collections import Counter
import time
warnings.filterwarnings('ignore')

# Param√®tres
EMBEDDINGS_PATH = "data/processed/news_embeddings.pt"
DATA_PATH = "data/processed/news_clean.csv"
OUTPUT_PATH = "data/processed/news_with_discourse.csv"
RANDOM_STATE = 42
SAMPLE_SIZE = 15000  # Travailler sur un √©chantillon pour les m√©thodes lentes

print("="*60)
print("üöÄ ANALYSE DES DISCOURS - VERSION OPTIMIS√âE")
print("="*60)

print("\nüì• Chargement des donn√©es...")
start_time = time.time()

# 1. Charger les donn√©es
embeddings = torch.load(EMBEDDINGS_PATH, map_location='cpu')
embeddings_np = embeddings.numpy()
df = pd.read_csv(DATA_PATH)

print(f"‚úÖ Embeddings: {embeddings_np.shape}")
print(f"‚úÖ Donn√©es: {df.shape}")
print(f"‚è±Ô∏è  Temps chargement: {time.time() - start_time:.1f}s")

# 2. √âCHANTILLONNAGE INTELLIGENT pour m√©thodes lentes
print(f"\nüéØ Cr√©ation √©chantillon ({SAMPLE_SIZE} articles)...")
if len(df) > SAMPLE_SIZE:
    # √âchantillon stratifi√© si possible (par source ou date)
    sample_idx = np.random.choice(len(df), SAMPLE_SIZE, replace=False)
    embeddings_sample = embeddings_np[sample_idx]
    df_sample = df.iloc[sample_idx].copy()
else:
    embeddings_sample = embeddings_np
    df_sample = df.copy()

print(f"   √âchantillon: {embeddings_sample.shape}")

# 3. M√âTHODE RAPIDE 1: MiniBatch K-means sur embeddings
print("\n" + "="*60)
print("1Ô∏è‚É£  M√âTHODE RAPIDE: MiniBatch K-means")
print("="*60)

start_time = time.time()

# Normalisation rapide
scaler = StandardScaler()
embeddings_scaled = scaler.fit_transform(embeddings_sample)

# MiniBatch K-means (beaucoup plus rapide)
mbkmeans = MiniBatchKMeans(
    n_clusters=5,  # Commencer avec 5 clusters
    random_state=RANDOM_STATE,
    batch_size=1000,
    n_init=3,
    max_iter=100
)

mbkmeans_labels = mbkmeans.fit_predict(embeddings_scaled)
df_sample['cluster_mbkmeans'] = mbkmeans_labels

# Calcul rapide silhouette (sur sous-√©chantillon)
sil_score = silhouette_score(embeddings_scaled[:2000], mbkmeans_labels[:2000])

print(f"‚úÖ MiniBatch K-means termin√©")
print(f"   ‚è±Ô∏è  Temps: {time.time() - start_time:.1f}s")
print(f"   üéØ Silhouette (√©chantillon): {sil_score:.3f}")
print(f"   üìä Distribution: {dict(Counter(mbkmeans_labels))}")

# 4. M√âTHODE RAPIDE 2: Clustering hi√©rarchique agglom√©ratif
print("\n" + "="*60)
print("2Ô∏è‚É£  M√âTHODE RAPIDE: Clustering Hi√©rarchique")
print("="*60)

start_time = time.time()

# R√©duction de dimension pour clustering hi√©rarchique
pca_fast = PCA(n_components=50)
embeddings_pca = pca_fast.fit_transform(embeddings_scaled)

# Clustering hi√©rarchique avec linkage 'ward' (efficace)
agglo = AgglomerativeClustering(
    n_clusters=4,
    linkage='ward',
    metric='euclidean'
)

agglo_labels = agglo.fit_predict(embeddings_pca)
df_sample['cluster_agglo'] = agglo_labels

print(f"‚úÖ Clustering hi√©rarchique termin√©")
print(f"   ‚è±Ô∏è  Temps: {time.time() - start_time:.1f}s")
print(f"   üìä Distribution: {dict(Counter(agglo_labels))}")

# 5. M√âTHODE RAPIDE 3: Topic Modeling avec NMF
print("\n" + "="*60)
print("3Ô∏è‚É£  M√âTHODE RAPIDE: Topic Modeling (NMF)")
print("="*60)

start_time = time.time()

print("   Extraction des mots-cl√©s...")
# Vectorizer rapide avec peu de features
vectorizer = CountVectorizer(
    max_features=1000,
    min_df=10,
    max_df=0.7,
    stop_words='english'
)

X_counts = vectorizer.fit_transform(df_sample['clean_text'].fillna(''))

# NMF rapide
from sklearn.decomposition import NMF
nmf = NMF(
    n_components=6,  # 6 topics
    random_state=RANDOM_STATE,
    max_iter=50,  # Moins d'it√©rations
    alpha_W=0.1
)

W = nmf.fit_transform(X_counts)
topic_labels = W.argmax(axis=1)
df_sample['topic_nmf'] = topic_labels

print(f"‚úÖ Topic Modeling termin√©")
print(f"   ‚è±Ô∏è  Temps: {time.time() - start_time:.1f}s")
print(f"   üìä Topics: {dict(Counter(topic_labels))}")

# Afficher les mots-cl√©s par topic
print("\n   üìù Top mots par topic:")
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(nmf.components_):
    top_words_idx = topic.argsort()[-8:][::-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"     Topic {topic_idx}: {', '.join(top_words[:5])}")

# 6. ANALYSE DE DENSIT√â (m√©thode rapide)
print("\n" + "="*60)
print("4Ô∏è‚É£  ANALYSE: D√©tection des Zones Denses")
print("="*60)

start_time = time.time()

# Utiliser Nearest Neighbors pour d√©tecter la densit√©
print("   Calcul des densit√©s locales...")
nn = NearestNeighbors(n_neighbors=50, metric='euclidean')
nn.fit(embeddings_pca)

# Distance aux k plus proches voisins
distances, _ = nn.kneighbors(embeddings_pca)
avg_distances = distances.mean(axis=1)

# Identifier les points denses (faible distance moyenne)
dense_threshold = np.percentile(avg_distances, 30)  # 30% les plus denses
is_dense = avg_distances < dense_threshold

df_sample['is_dense_region'] = is_dense

print(f"‚úÖ Analyse densit√© termin√©e")
print(f"   ‚è±Ô∏è  Temps: {time.time() - start_time:.1f}s")
print(f"   üìä Zones denses: {is_dense.sum()} points ({(is_dense.sum()/len(df_sample)*100):.1f}%)")

# 7. ANALYSE DES EXTR√äMES (m√©thode rapide)
print("\n" + "="*60)
print("5Ô∏è‚É£  ANALYSE: D√©tection des Discours Extr√™mes")
print("="*60)

start_time = time.time()

# M√©thode simple: distance au centre
center = embeddings_scaled.mean(axis=0)
distances_to_center = np.linalg.norm(embeddings_scaled - center, axis=1)

# Identifier les extr√™mes (loin du centre)
extreme_threshold = np.percentile(distances_to_center, 90)  # Top 10% les plus √©loign√©s
is_extreme = distances_to_center > extreme_threshold

df_sample['is_extreme'] = is_extreme
df_sample['distance_to_center'] = distances_to_center

print(f"‚úÖ D√©tection extr√™mes termin√©e")
print(f"   ‚è±Ô∏è  Temps: {time.time() - start_time:.1f}s")
print(f"   üìä Discours extr√™mes: {is_extreme.sum()} ({(is_extreme.sum()/len(df_sample)*100):.1f}%)")

# 8. VISUALISATIONS RAPIDES
print("\n" + "="*60)
print("üé® VISUALISATIONS")
print("="*60)

print("   G√©n√©ration des visualisations...")
start_time = time.time()

# PCA pour visualisation
pca_vis = PCA(n_components=2)
embeddings_2d = pca_vis.fit_transform(embeddings_scaled)

fig = plt.figure(figsize=(18, 12))

# Subplot 1: MiniBatch K-means
ax1 = plt.subplot(2, 3, 1)
scatter1 = ax1.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                      c=df_sample['cluster_mbkmeans'], cmap='tab10',
                      alpha=0.5, s=10)
ax1.set_title(f'MiniBatch K-means (5 clusters)\nSilhouette: {sil_score:.3f}')
ax1.set_xlabel('PC1')
ax1.set_ylabel('PC2')
plt.colorbar(scatter1, ax=ax1)

# Subplot 2: Clustering hi√©rarchique
ax2 = plt.subplot(2, 3, 2)
scatter2 = ax2.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                      c=df_sample['cluster_agglo'], cmap='Set2',
                      alpha=0.5, s=10)
ax2.set_title('Clustering Hi√©rarchique\n(4 clusters)')
ax2.set_xlabel('PC1')
ax2.set_ylabel('PC2')
plt.colorbar(scatter2, ax=ax2)

# Subplot 3: Topics NMF
ax3 = plt.subplot(2, 3, 3)
scatter3 = ax3.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
                      c=df_sample['topic_nmf'], cmap='tab20',
                      alpha=0.5, s=10)
ax3.set_title(f'Topic Modeling (NMF)\n{df_sample["topic_nmf"].nunique()} topics')
ax3.set_xlabel('PC1')
ax3.set_ylabel('PC2')
plt.colorbar(scatter3, ax=ax3)

# Subplot 4: Zones denses
ax4 = plt.subplot(2, 3, 4)
colors = ['blue' if not dense else 'red' for dense in df_sample['is_dense_region']]
ax4.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
           c=colors, alpha=0.3, s=5)
ax4.set_title(f'Zones Denses (rouge)\n{df_sample["is_dense_region"].sum()} points')
ax4.set_xlabel('PC1')
ax4.set_ylabel('PC2')

# Subplot 5: Discours extr√™mes
ax5 = plt.subplot(2, 3, 5)
colors = ['blue' if not extreme else 'red' for extreme in df_sample['is_extreme']]
ax5.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1],
           c=colors, alpha=0.3, s=5)
ax5.set_title(f'Discours Extr√™mes (rouge)\n{df_sample["is_extreme"].sum()} points')
ax5.set_xlabel('PC1')
ax5.set_ylabel('PC2')

# Subplot 6: R√©sum√© statistique
ax6 = plt.subplot(2, 3, 6)
ax6.axis('off')

summary_text = f"R√âSUM√â DE L'ANALYSE\n\n"
summary_text += f"√âchantillon: {len(df_sample):,} articles\n"
summary_text += f"Period: "
if 'date' in df_sample.columns:
    summary_text += f"{df_sample['date'].min()[:7]} √† {df_sample['date'].max()[:7]}\n\n"
else:
    summary_text += "N/A\n\n"

summary_text += f"MiniBatch K-means:\n"
summary_text += f"  ‚Ä¢ 5 clusters\n"
summary_text += f"  ‚Ä¢ Silhouette: {sil_score:.3f}\n\n"

summary_text += f"Clustering Hi√©rarchique:\n"
summary_text += f"  ‚Ä¢ 4 clusters\n\n"

summary_text += f"Topic Modeling:\n"
summary_text += f"  ‚Ä¢ {df_sample['topic_nmf'].nunique()} topics\n\n"

summary_text += f"Zones Denses:\n"
summary_text += f"  ‚Ä¢ {df_sample['is_dense_region'].sum():,} points\n"
summary_text += f"  ‚Ä¢ {(df_sample['is_dense_region'].sum()/len(df_sample)*100):.1f}%\n\n"

summary_text += f"Discours Extr√™mes:\n"
summary_text += f"  ‚Ä¢ {df_sample['is_extreme'].sum():,} points\n"
summary_text += f"  ‚Ä¢ {(df_sample['is_extreme'].sum()/len(df_sample)*100):.1f}%"

ax6.text(0.05, 0.95, summary_text, transform=ax6.transAxes,
        fontsize=10, verticalalignment='top',
        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))

plt.tight_layout()
plt.savefig("data/processed/fast_analysis_results.png", dpi=120, bbox_inches='tight')
print(f"   ‚úÖ Visualisation sauvegard√©e: data/processed/fast_analysis_results.png")
print(f"   ‚è±Ô∏è  Temps visualisation: {time.time() - start_time:.1f}s")

# 9. PROPAGATION AU DATASET COMPLET (m√©thode intelligente)
print("\n" + "="*60)
print("üåç PROPAGATION AU DATASET COMPLET")
print("="*60)

print("   Propagation des clusters √† tous les articles...")

# Pour K-means: entra√Æner sur √©chantillon, pr√©dire sur tout
if len(df) > SAMPLE_SIZE:
    # Entra√Æner K-means sur l'√©chantillon
    kmeans_full = KMeans(
        n_clusters=5,
        random_state=RANDOM_STATE,
        n_init=3
    )
    kmeans_full.fit(embeddings_scaled)
    
    # Pr√©dire sur tout le dataset
    print("   Pr√©diction sur les 35k articles...")
    all_embeddings_scaled = scaler.transform(embeddings_np)
    all_clusters = kmeans_full.predict(all_embeddings_scaled)
    df['cluster_kmeans_full'] = all_clusters
    
    print(f"   ‚úÖ Clusters assign√©s √† tous les articles")
    print(f"   üìä Distribution: {dict(Counter(all_clusters))}")

# 10. SAUVEGARDE DES R√âSULTATS
print("\n" + "="*60)
print("üíæ SAUVEGARDE")
print("="*60)

# Sauvegarder l'analyse d√©taill√©e de l'√©chantillon
df_sample.to_csv(OUTPUT_PATH.replace('.csv', '_sample_detailed.csv'), index=False)
print(f"‚úÖ Analyse √©chantillon: {OUTPUT_PATH.replace('.csv', '_sample_detailed.csv')}")

# Sauvegarder les clusters pour tout le dataset
if 'cluster_kmeans_full' in df.columns:
    df[['title', 'clean_text', 'date', 'source', 'cluster_kmeans_full']].to_csv(OUTPUT_PATH, index=False)
    print(f"‚úÖ Clusters dataset complet: {OUTPUT_PATH}")

# G√©n√©rer un rapport rapide
report_text = f"""ANALYSE DES DISCOURS M√âDIATIQUES - RAPPORT RAPIDE
{'='*60}

DONN√âES ANALYS√âES:
‚Ä¢ Total articles: {len(df):,}
‚Ä¢ √âchantillon analys√©: {len(df_sample):,}
‚Ä¢ P√©riode: {df['date'].min()[:10] if 'date' in df.columns else 'N/A'} √† {df['date'].max()[:10] if 'date' in df.columns else 'N/A'}

R√âSULTATS CL√âS:

1. STRUCTURE DES DISCOURS:
‚Ä¢ MiniBatch K-means r√©v√®le {df_sample['cluster_mbkmeans'].nunique()} clusters
‚Ä¢ Score silhouette: {sil_score:.3f} (structure faible mais discernable)
‚Ä¢ Distribution: {dict(Counter(df_sample['cluster_mbkmeans']))}

2. TOPICS IDENTIFI√âS (NMF):
‚Ä¢ {df_sample['topic_nmf'].nunique()} topics principaux
‚Ä¢ Distribution: {dict(Counter(df_sample['topic_nmf']))}

3. ZONES DE CONCENTRATION:
‚Ä¢ {(df_sample['is_dense_region'].sum()/len(df_sample)*100):.1f}% des discours dans zones denses
‚Ä¢ Indique des th√®mes r√©currents/pr√©dominants

4. DISCOURS EXTR√äMES:
‚Ä¢ {(df_sample['is_extreme'].sum()/len(df_sample)*100):.1f}% des discours identifi√©s comme 'extr√™mes'
‚Ä¢ Ces articles sont s√©mantiquement √©loign√©s du discours m√©dian

INTERPR√âTATION:
Les discours m√©diatiques forment un continuum avec quelques p√¥les de concentration.
L'absence de clusters nets sugg√®re une relative homog√©n√©it√© des discours
ou la n√©cessit√© d'analyses plus fines (par source, par p√©riode).

NEXT STEPS:
1. Analyse comparative par source m√©diatique
2. √âvolution temporelle des topics
3. Analyse sentiment par cluster
"""

with open("data/processed/quick_analysis_report.txt", "w") as f:
    f.write(report_text)

print(f"‚úÖ Rapport g√©n√©r√©: data/processed/quick_analysis_report.txt")

print("\n" + "="*60)
print("üéâ ANALYSE TERMIN√âE!")
print("="*60)
print(f"\nüìä R√âCAPITULATIF:")
print(f"   ‚Ä¢ Articles analys√©s: {len(df):,}")
print(f"   ‚Ä¢ M√©thodes test√©es: 5")
print(f"   ‚Ä¢ Clusters identifi√©s: {df_sample['cluster_mbkmeans'].nunique()}")
print(f"   ‚Ä¢ Topics d√©tect√©s: {df_sample['topic_nmf'].nunique()}")
print(f"   ‚Ä¢ Visualisations: 1")
print(f"   ‚Ä¢ Rapports: 2")
print(f"\nüìÅ FICHIERS G√âN√âR√âS:")
print(f"   ‚Ä¢ data/processed/fast_analysis_results.png")
print(f"   ‚Ä¢ data/processed/news_with_discourse.csv")
print(f"   ‚Ä¢ data/processed/quick_analysis_report.txt")